"""
LSTM Î™®Îç∏ 2: Î≥ÄÎèôÏÑ± ÏòàÏ∏° Î™®Îç∏ (Î¶¨Ìå©ÌÜ†ÎßÅ Î≤ÑÏ†Ñ)
Ïª§Ìîº Í∞ÄÍ≤©Ïùò Î≥ÄÎèôÏÑ±ÏùÑ ÏòàÏ∏°ÌïòÏó¨ Î¶¨Ïä§ÌÅ¨ Í¥ÄÎ¶¨Ïóê ÌôúÏö©ÌïòÎäî Î™®Îç∏

Ï£ºÏöî Í∏∞Îä•:
- Í±∞ÏãúÍ≤ΩÏ†ú Î∞è Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞Î•º ÌôúÏö©Ìïú Î≥ÄÎèôÏÑ± ÏòàÏ∏°
- EntMax Attention Î©îÏª§ÎãàÏ¶ò Ï†ÅÏö©
- Ï†ïÏ†Å/ÎèôÏ†Å ÌîºÏ≤ò Î∂ÑÎ¶¨ Ï≤òÎ¶¨
- ÏãúÍ≥ÑÏó¥ ÍµêÏ∞®Í≤ÄÏ¶ù
"""

import pandas as pd
import numpy as np
import os
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error
from entmax import Entmax15

plt.rcParams['font.family'] ='Malgun Gothic'
plt.rcParams['axes.unicode_minus'] =False

# ÏÑ§Ï†ï
plt.rcParams['font.family'] = 'DejaVu Sans'  # ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï Ï†úÍ±∞ (Î≤îÏö©ÏÑ±)
plt.rcParams['axes.unicode_minus'] = False
device = 'cuda' if torch.cuda.is_available() else 'cpu'

def load_eco_data(data_path=None):
    """Í±∞ÏãúÍ≤ΩÏ†ú Î∞è Ïª§Ìîº Í∞ÄÍ≤© ÌÜµÌï© Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú"""
    if data_path is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        app_dir = os.path.abspath(os.path.join(current_dir, '../../../'))
        data_path = os.path.join(app_dir, 'data', 'input', 'Í±∞ÏãúÍ≤ΩÏ†úÎ∞èÏª§ÌîºÍ∞ÄÍ≤©ÌÜµÌï©Îç∞Ïù¥ÌÑ∞.csv')
        
        if not os.path.exists(data_path):
            root_dir = os.path.abspath(os.path.join(current_dir, '../../../../'))
            data_path = os.path.join(root_dir, 'app', 'data', 'input', 'Í±∞ÏãúÍ≤ΩÏ†úÎ∞èÏª§ÌîºÍ∞ÄÍ≤©ÌÜµÌï©Îç∞Ïù¥ÌÑ∞.csv')

    print(f"‚è≥ Í≤ΩÏ†ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë: {data_path}")
    
    try:
        df = pd.read_csv(data_path)
        print(f"‚úÖ Í≤ΩÏ†ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏÑ±Í≥µ: {df.shape}")
        return df
    except Exception as e:
        print(f"‚ùå Í≤ΩÏ†ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
        raise

def load_weather_data(data_path=None):
    """Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú"""
    if data_path is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        app_dir = os.path.abspath(os.path.join(current_dir, '../../../'))
        data_path = os.path.join(app_dir, 'data', 'input', 'ÎπÑÏàòÌôïÍ∏∞ÌèâÍ∑†Ïª§ÌîºÍ∞ÄÍ≤©ÌÜµÌï©Îç∞Ïù¥ÌÑ∞.csv')
        
        if not os.path.exists(data_path):
            root_dir = os.path.abspath(os.path.join(current_dir, '../../../../'))
            data_path = os.path.join(root_dir, 'app', 'data', 'input', 'ÎπÑÏàòÌôïÍ∏∞ÌèâÍ∑†Ïª§ÌîºÍ∞ÄÍ≤©ÌÜµÌï©Îç∞Ïù¥ÌÑ∞.csv')

    print(f"‚è≥ Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë: {data_path}")
    
    try:
        df = pd.read_csv(data_path)
        print(f"‚úÖ Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏÑ±Í≥µ: {df.shape}")
        return df
    except Exception as e:
        print(f"‚ùå Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ïã§Ìå®: {e}")
        raise

def create_features(df):
    """ÌååÏÉù Î≥ÄÏàò ÏÉùÏÑ±"""
    print("‚è≥ ÌååÏÉù Î≥ÄÏàò ÏÉùÏÑ± Ï§ë...")
    
    # Í∏∞Î≥∏ Î≥ÄÎèôÏÑ± Í¥ÄÎ†® ÌîºÏ≤ò
    df['abs_return'] = df['Coffee_Price_Return'].abs()
    df["log_return"] = np.log(df["Coffee_Price"]) - np.log(df["Coffee_Price"].shift(1))
    
    # ÌÉÄÍ≤ü Î≥ÄÏàò (shitÏóê ÏùòÌï¥ÏÑú ÎØ∏Îûò Îç∞Ïù¥ÌÑ∞Î•º Ï∞∏Í≥†ÌïòÎäî Í≤ΩÌñ•Ïù¥ ÏûàÏùå)
    df["target_volatility_14d"] = df["log_return"].rolling(window=14).std()\
        # .shift(-13)
    
    # Î≥ÄÎèôÏÑ± ÌîºÏ≤òÎì§
    df['volatility_5d'] = df['Coffee_Price_Return'].rolling(window=5).std()
    df['volatility_10d'] = df['Coffee_Price_Return'].rolling(window=10).std()
    
    # Î™®Î©òÌÖÄ ÌîºÏ≤òÎì§
    df['momentum_1d'] = df['Coffee_Price'].diff(1)
    df['momentum_3d'] = df['Coffee_Price'].diff(3)
    df['momentum_5d'] = df['Coffee_Price'] - df['Coffee_Price'].shift(5)
    
    # Î≥ºÎ¶∞Ï†Ä Î∞¥Îìú Î∞è Í∏∞ÌÉÄ ÌîºÏ≤ò
    rolling_mean = df['Coffee_Price'].rolling(window=20).mean()
    rolling_std = df['Coffee_Price'].rolling(window=20).std()
    df['bollinger_width'] = (2 * rolling_std) / rolling_mean
    
    df['return_zscore'] = (df['Coffee_Price_Return'] - df['Coffee_Price_Return'].rolling(20).mean()) / \
                          (df['Coffee_Price_Return'].rolling(20).std() + 1e-6)
    
    df['volatility_ratio'] = df['volatility_5d'] / (df['volatility_10d'] + 1e-6)
    
    print(f"‚úÖ ÌååÏÉù Î≥ÄÏàò ÏÉùÏÑ± ÏôÑÎ£å: {df.shape}")
    return df

def prepare_data():
    """Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨"""
    print("üöÄ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ÏãúÏûë")
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    df = load_eco_data()
    we = load_weather_data()
    
    # ÌååÏÉù Î≥ÄÏàò ÏÉùÏÑ±
    df = create_features(df)
    
    # Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞ Î≥ëÌï© (Ï§ëÎ≥µ Ïª¨Îüº Ï†úÍ±∞)
    weather_cols_to_drop = ['Coffee_Price', 'Coffee_Price_Return', 'Crude_Oil_Price', 
                           'USD_KRW', 'USD_BRL', 'USD_COP']
    we = we.drop(columns=[col for col in weather_cols_to_drop if col in we.columns])
    df = pd.merge(df, we, on='Date', how='left')
    
    # Í≤∞Ï∏°Ïπò Ï†úÍ±∞
    df = df.dropna()
    print(f"‚úÖ ÏµúÏ¢Ö Îç∞Ïù¥ÌÑ∞ ÌòïÌÉú: {df.shape}")
    
    return df

class MultiStepTimeSeriesDataset(torch.utils.data.Dataset):
    """ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, X, y, window_size, step, static_feat_idx):
        self.data = []
        self.labels = []
        self.static_feats = []
        self.seq_feat_idx = [i for i in range(X.shape[1]) if i not in static_feat_idx]

        for i in range(0, len(X) - window_size, step):
            x_seq = X[i:i+window_size, self.seq_feat_idx]
            x_static = X[i, static_feat_idx]
            y_target = y[i + window_size]

            if np.isnan(y_target):
                continue

            self.data.append(x_seq)
            self.static_feats.append(x_static)
            self.labels.append(y_target)

        self.data = np.array(self.data)
        self.static_feats = np.array(self.static_feats)
        self.labels = np.array(self.labels)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        x_seq = torch.tensor(self.data[idx], dtype=torch.float32)
        x_static = torch.tensor(self.static_feats[idx], dtype=torch.float32)
        y = torch.tensor(self.labels[idx], dtype=torch.float32)
        return x_seq, x_static, y

class EntmaxAttention(nn.Module):
    """EntMax Attention Î©îÏª§ÎãàÏ¶ò"""
    
    def __init__(self, hidden_size, attn_dim=128):
        super().__init__()
        self.score_layer = nn.Sequential(
            nn.Linear(hidden_size, attn_dim),
            nn.Tanh(),
            nn.Linear(attn_dim, 1)
        )
        self.entmax = Entmax15(dim=1)

    def forward(self, lstm_output):
        scores = self.score_layer(lstm_output).squeeze(-1)
        weights = self.entmax(scores)
        context = torch.sum(lstm_output * weights.unsqueeze(-1), dim=1)
        return context, weights

class AttentionLSTMModel(nn.Module):
    """Attention Í∏∞Î∞ò LSTM Î≥ÄÎèôÏÑ± ÏòàÏ∏° Î™®Îç∏"""
    
    def __init__(self, input_size, hidden_size=128, num_layers=1, dropout=0.3, static_feat_dim=9):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM Î†àÏù¥Ïñ¥
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )

        # Attention Î©îÏª§ÎãàÏ¶ò
        self.attention = EntmaxAttention(hidden_size)

        # Gate Î©îÏª§ÎãàÏ¶ò
        self.gate = nn.Sequential(
            nn.Linear(hidden_size * 2, 1),
            nn.Sigmoid()
        )

        # Ï†ïÏ†Å ÌîºÏ≤ò Ïù∏ÏΩîÎçî
        self.static_encoder = nn.Sequential(
            nn.Linear(static_feat_dim, 32),
            nn.ReLU(),
            nn.LayerNorm(32),
            nn.Dropout(0.2),
            nn.Linear(32, 64),
            nn.ReLU()
        )

        # ÏµúÏ¢Ö Ï∂úÎ†• Î†àÏù¥Ïñ¥
        self.fc = nn.Sequential(
            nn.Linear(hidden_size + 64, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, x_seq, x_static, hidden_states=None):
        batch_size = x_seq.size(0)

        # Ï¥àÍ∏∞ hidden state ÏÑ§Ï†ï
        if hidden_states is None:
            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x_seq.device)
            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x_seq.device)
            hidden_states = (h0, c0)

        # LSTM ÌÜµÍ≥º
        lstm_out, _ = self.lstm(x_seq, hidden_states)

        # Attention Ï†ÅÏö©
        context, attn_weights = self.attention(lstm_out)
        last_hidden = lstm_out[:, -1, :]
        
        # GateÎ•º ÌÜµÌïú Ï†ïÎ≥¥ Í≤∞Ìï©
        combined = torch.cat([context, last_hidden], dim=1)
        alpha = self.gate(combined)
        fused = alpha * context + (1 - alpha) * last_hidden

        # Ï†ïÏ†Å ÌîºÏ≤ò Ïù∏ÏΩîÎî© Î∞è Í≤∞Ìï©
        static_encoded = self.static_encoder(x_static)
        fused_with_static = torch.cat([fused, static_encoded], dim=1)

        # ÏµúÏ¢Ö ÏòàÏ∏°
        out = self.fc(fused_with_static).squeeze(-1)
        return out, attn_weights

def weighted_mse_loss(y_pred, y_true, temp=5.0):
    """Í∞ÄÏ§ë MSE ÏÜêÏã§ Ìï®Ïàò"""
    sample_losses = (y_pred - y_true) ** 2
    weights = torch.softmax(sample_losses * temp, dim=0)
    weighted_loss = torch.sum(weights * sample_losses)
    return weighted_loss

def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):
    """Î™®Îç∏ ÌõàÎ†®"""
    print(f"‚è≥ Î™®Îç∏ ÌõàÎ†® ÏãúÏûë (ÏóêÌè≠: {num_epochs})")
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    train_losses = []
    val_losses = []
    
    for epoch in range(num_epochs):
        # ÌõàÎ†® Î™®Îìú
        model.train()
        epoch_loss = 0.0
        
        for x_seq, x_static, y_batch in train_loader:
            x_seq, x_static, y_batch = x_seq.to(device), x_static.to(device), y_batch.to(device)
            
            optimizer.zero_grad()
            y_pred, _ = model(x_seq, x_static)
            loss = weighted_mse_loss(y_pred, y_batch, temp=5.0)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_train_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Í≤ÄÏ¶ù Î™®Îìú
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for x_seq, x_static, y_batch in val_loader:
                x_seq, x_static, y_batch = x_seq.to(device), x_static.to(device), y_batch.to(device)
                y_pred, _ = model(x_seq, x_static)
                val_loss += nn.MSELoss()(y_pred, y_batch).item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        scheduler.step(avg_val_loss)
        
        if epoch % 10 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}] | "
                  f"Train Loss: {avg_train_loss:.4f} | "
                  f"Val Loss: {avg_val_loss:.4f}")
    
    print("‚úÖ Î™®Îç∏ ÌõàÎ†® ÏôÑÎ£å")
    return train_losses, val_losses

def cross_validate_model(X_all, y_all, static_feat_idx, data_window=100, step=1):
    """ÏãúÍ≥ÑÏó¥ ÍµêÏ∞®Í≤ÄÏ¶ù"""
    print("‚è≥ ÍµêÏ∞®Í≤ÄÏ¶ù ÏãúÏûë")
    
    tscv = TimeSeriesSplit(n_splits=5)
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_all)):
        print(f"\n===== Fold {fold + 1} =====")
        
        X_train_fold = X_all[train_idx]
        y_train_fold = y_all[train_idx]
        X_val_fold = X_all[val_idx]
        y_val_fold = y_all[val_idx]
        
        # Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±
        train_dataset = MultiStepTimeSeriesDataset(
            X_train_fold, y_train_fold, data_window, step, static_feat_idx
        )
        val_dataset = MultiStepTimeSeriesDataset(
            X_val_fold, y_val_fold, data_window, step, static_feat_idx
        )
        
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)
        
        # Î™®Îç∏ ÏÉùÏÑ± Î∞è ÌõàÎ†®
        model = AttentionLSTMModel(
            input_size=X_all.shape[1] - len(static_feat_idx),
            static_feat_dim=len(static_feat_idx)
        ).to(device)
        
        _, val_losses = train_model(model, train_loader, val_loader, num_epochs=5)
        fold_results.append(min(val_losses))
    
    print(f"\n‚úÖ ÍµêÏ∞®Í≤ÄÏ¶ù ÏôÑÎ£å")
    print(f"FoldÎ≥Ñ ÏµúÍ≥† ÏÑ±Îä•: {fold_results}")
    print(f"ÌèâÍ∑† Í≤ÄÏ¶ù ÏÑ±Îä•: {np.mean(fold_results):.4f}")
    
    return fold_results

def evaluate_model(model, test_loader, scaler, test_df, target_col="target_volatility_14d"):
    """Î™®Îç∏ ÌèâÍ∞Ä Î∞è Í≤∞Í≥º Î∂ÑÏÑù"""
    print("‚è≥ Î™®Îç∏ ÌèâÍ∞Ä Ï§ë...")
    
    model.eval()
    predictions = []
    true_values = []
    date_ranges = []
    
    target_idx = test_df.columns.get_loc(target_col)
    data_window = 100
    step = 1
    
    with torch.no_grad():
        for batch_idx, (x_seq, x_static, y_true) in enumerate(test_loader):
            x_seq = x_seq.to(device)
            x_static = x_static.to(device)
            
            y_pred, _ = model(x_seq, x_static)
            y_pred = y_pred.cpu().numpy()
            y_true = y_true.cpu().numpy()
            
            for i in range(x_seq.size(0)):
                global_idx = batch_idx * test_loader.batch_size + i
                target_idx_in_df = global_idx * step + data_window
                
                if target_idx_in_df >= len(test_df):
                    continue
                
                # Ïó≠Ï†ïÍ∑úÌôî
                dummy_pred = np.zeros((1, test_df.shape[1]))
                dummy_true = np.zeros((1, test_df.shape[1]))
                dummy_pred[0, target_idx] = y_pred[i]
                dummy_true[0, target_idx] = y_true[i]
                
                y_pred_inv = scaler.inverse_transform(dummy_pred)[0, target_idx]
                y_true_inv = scaler.inverse_transform(dummy_true)[0, target_idx]
                
                predictions.append(y_pred_inv)
                true_values.append(y_true_inv)
                date_ranges.append(test_df.index[target_idx_in_df])
    
    # ÏÑ±Îä• ÏßÄÌëú Í≥ÑÏÇ∞
    rmse = np.sqrt(mean_squared_error(true_values, predictions))
    mae = mean_absolute_error(true_values, predictions)
    
    print(f"‚úÖ ÌèâÍ∞Ä ÏôÑÎ£å - RMSE: {rmse:.4f}, MAE: {mae:.4f}")
    
    return predictions, true_values, date_ranges, rmse, mae

def visualize_results(predictions, true_values, date_ranges, title="Volatility Prediction Results"):
    """Í≤∞Í≥º ÏãúÍ∞ÅÌôî"""
    pred_series = pd.Series(predictions, index=date_ranges)
    true_series = pd.Series(true_values, index=date_ranges)
    
    plt.figure(figsize=(15, 8))
    plt.plot(true_series.sort_index(), label='Actual Volatility', linewidth=2)
    plt.plot(pred_series.sort_index(), label='Predicted Volatility', linestyle='--', linewidth=2)
    plt.title(title, fontsize=16)
    plt.xlabel('Date', fontsize=12)
    plt.ylabel('Volatility', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=12)
    plt.tight_layout()
    plt.show()

def simulate_price_curve(start_date, predicted_volatility, base_price, num_days=14):
    """Î≥ÄÎèôÏÑ± Í∏∞Î∞ò Í∞ÄÍ≤© ÏãúÎÆ¨Î†àÏù¥ÏÖò"""
    np.random.seed(42)
    simulated_returns = np.random.normal(loc=0.0, scale=predicted_volatility, size=num_days)
    prices = base_price * np.exp(np.cumsum(simulated_returns))
    date_range = pd.date_range(start=start_date, periods=num_days)
    return pd.Series(prices, index=date_range)

def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    print("üöÄ LSTM Î≥ÄÎèôÏÑ± ÏòàÏ∏° Î™®Îç∏ ÏãúÏûë")
    
    # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
    df = prepare_data()
    
    # ÎÇ†Ïßú Ï≤òÎ¶¨ Î∞è Ïù∏Îç±Ïä§ ÏÑ§Ï†ï
    df['Date'] = pd.to_datetime(df['Date'])
    df.set_index('Date', inplace=True)
    
    # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
    n = len(df)
    train_size = int(n * 0.9)
    train_df = df.iloc[:train_size].copy()
    test_df = df.iloc[train_size:].copy()
    
    # Ï†ïÍ∑úÌôî (ÏùºÎ∂Ä Ïª¨Îüº Ï†úÏô∏)
    scaler = MinMaxScaler()
    
    # Ï†úÏô∏Ìï† Ïª¨ÎüºÎì§ Î∞±ÏóÖ
    return_train = train_df["Coffee_Price_Return"].copy()
    return_test = test_df["Coffee_Price_Return"].copy()
    log_return_train = train_df["log_return"].copy()
    log_return_test = test_df["log_return"].copy()
    
    # Ï†ïÍ∑úÌôî Ï†ÅÏö©
    train_df_scaled = pd.DataFrame(
        scaler.fit_transform(train_df),
        columns=train_df.columns,
        index=train_df.index
    )
    test_df_scaled = pd.DataFrame(
        scaler.transform(test_df),
        columns=test_df.columns,
        index=test_df.index
    )
    
    # Ï†úÏô∏ Ïª¨ÎüºÎì§ Î≥µÏõê
    train_df_scaled["Coffee_Price_Return"] = return_train
    test_df_scaled["Coffee_Price_Return"] = return_test
    train_df_scaled["log_return"] = log_return_train
    test_df_scaled["log_return"] = log_return_test
    
    # Ï†ïÏ†Å ÌîºÏ≤ò Ïù∏Îç±Ïä§ (ÎßàÏßÄÎßâ 9Í∞ú)
    static_feat_idx = list(range(train_df_scaled.shape[1] - 9, train_df_scaled.shape[1]))
    
    # Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±
    data_window = 100
    step = 1
    target_col = "target_volatility_14d"
    
    X_train = train_df_scaled.values
    y_train = train_df_scaled[target_col].values
    X_test = test_df_scaled.values
    y_test = test_df_scaled[target_col].values
    
    # ÍµêÏ∞®Í≤ÄÏ¶ù (ÏÑ†ÌÉùÏÇ¨Ìï≠)
    print("ÍµêÏ∞®Í≤ÄÏ¶ùÏùÑ ÏàòÌñâÌïòÏãúÍ≤†ÏäµÎãàÍπå? (y/n): ", end="")
    if input().lower() == 'y':
        cross_validate_model(X_train, y_train, static_feat_idx, data_window, step)
    
    # ÏµúÏ¢Ö Î™®Îç∏ ÌõàÎ†®
    train_dataset = MultiStepTimeSeriesDataset(X_train, y_train, data_window, step, static_feat_idx)
    test_dataset = MultiStepTimeSeriesDataset(X_test, y_test, data_window, step, static_feat_idx)
    
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # Î™®Îç∏ ÏÉùÏÑ± Î∞è ÌõàÎ†®
    model = AttentionLSTMModel(
        input_size=X_train.shape[1] - len(static_feat_idx),
        static_feat_dim=len(static_feat_idx)
    ).to(device)
    
    train_losses, val_losses = train_model(model, train_loader, test_loader, num_epochs=20)
    
    # Î™®Îç∏ ÌèâÍ∞Ä
    predictions, true_values, date_ranges, rmse, mae = evaluate_model(
        model, test_loader, scaler, test_df, target_col
    )
    
    # Í≤∞Í≥º ÏãúÍ∞ÅÌôî
    visualize_results(predictions, true_values, date_ranges)
    
    # ÏÉòÌîå Í∞ÄÍ≤© ÏãúÎÆ¨Î†àÏù¥ÏÖò
    if len(predictions) > 0:
        sample_date = date_ranges[0]
        sample_vol = predictions[0]
        sample_price = test_df.loc[sample_date, 'Coffee_Price'] if sample_date in test_df.index else 200
        
        sim_prices = simulate_price_curve(sample_date, sample_vol, sample_price)
        
        plt.figure(figsize=(12, 6))
        plt.plot(sim_prices.index, sim_prices.values, linewidth=2)
        plt.title(f"Simulated Price Curve (Start Date: {sample_date.date()}, Volatility: {sample_vol:.4f})")
        plt.xlabel('Date')
        plt.ylabel('Simulated Price')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
    
    print(f"üèÅ Î∂ÑÏÑù ÏôÑÎ£å - ÏµúÏ¢Ö ÏÑ±Îä•: RMSE={rmse:.4f}, MAE={mae:.4f}")

if __name__ == "__main__":
    main()