import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from config import DEVICE
from data_loader import load_data
from feature_engineering import create_features
from dataset import MultiStepTimeSeriesDataset
from model import AttentionLSTMModel
from train import train_model
from evaluate import evaluate
from visualize import plot_prediction
from simulate import simulate_price_curve
import torch
from torch.utils.data import DataLoader


def main():
    # ë°ì´í„° ë¡œë”© ë° ë³‘í•©
    df_macro, df_weather = load_data()
    df = create_features(df_macro)
    df_weather = df_weather.drop(columns=[col for col in df_weather.columns if col in df.columns and col != 'Date'])
    df = pd.merge(df, df_weather, on='Date', how='left')
    df.dropna(inplace=True)

    # ì¸ë±ìŠ¤ ì²˜ë¦¬
    df['Date'] = pd.to_datetime(df['Date'])
    df.set_index('Date', inplace=True)

    # ë°ì´í„° ë¶„í• 
    train_size = int(len(df) * 0.9)
    train_df = df.iloc[:train_size].copy()
    test_df = df.iloc[train_size:].copy()

    # ì •ê·œí™”
    scaler = MinMaxScaler()
    y_col = 'target_volatility_14d'
    return_backup = train_df['Coffee_Price_Return'].copy(), test_df['Coffee_Price_Return'].copy()
    log_return_backup = train_df['log_return'].copy(), test_df['log_return'].copy()

    train_scaled = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns, index=train_df.index)
    test_scaled = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns, index=test_df.index)

    train_scaled['Coffee_Price_Return'] = return_backup[0]
    test_scaled['Coffee_Price_Return'] = return_backup[1]
    train_scaled['log_return'] = log_return_backup[0]
    test_scaled['log_return'] = log_return_backup[1]

    # ì •ì  í”¼ì²˜ ì¸ë±ìŠ¤ (ë§ˆì§€ë§‰ 9ê°œ)
    static_idx = list(range(train_scaled.shape[1] - 9, train_scaled.shape[1]))
    
    X_train = train_scaled.values
    y_train = train_scaled[y_col].values
    X_test = test_scaled.values
    y_test = test_scaled[y_col].values

    # ë°ì´í„°ì…‹ ë° ë¡œë”
    window = 100
    step = 1
    train_ds = MultiStepTimeSeriesDataset(X_train, y_train, window, step, static_idx)
    test_ds = MultiStepTimeSeriesDataset(X_test, y_test, window, step, static_idx)
    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)

    # ëª¨ë¸ í•™ìŠµ
    model = AttentionLSTMModel(input_size=X_train.shape[1] - len(static_idx), static_dim=len(static_idx)).to(DEVICE)
    train_model(model, train_loader, test_loader, epochs=20)

    # ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘
    model.eval()
    predictions, targets, dates = [], [], []
    with torch.no_grad():
        for i, (x_seq, x_static, y) in enumerate(test_loader):
            x_seq, x_static = x_seq.to(DEVICE), x_static.to(DEVICE)
            out = model(x_seq, x_static).cpu().numpy()
            y = y.cpu().numpy()
            for j in range(len(out)):
                idx = i * test_loader.batch_size + j
                global_idx = idx * step + window
                if global_idx < len(test_df):
                    pred_scaled = np.zeros((1, test_df.shape[1]))
                    true_scaled = np.zeros((1, test_df.shape[1]))
                    target_idx = test_df.columns.get_loc(y_col)
                    pred_scaled[0, target_idx] = out[j]
                    true_scaled[0, target_idx] = y[j]
                    predictions.append(scaler.inverse_transform(pred_scaled)[0, target_idx])
                    targets.append(scaler.inverse_transform(true_scaled)[0, target_idx])
                    dates.append(test_df.index[global_idx])

    # í‰ê°€ ë° ì‹œê°í™”
    metrics = evaluate(predictions, targets)
    print("\n[Final Evaluation] RMSE: {:.4f}, MAE: {:.4f}".format(metrics['rmse'], metrics['mae']))
    plot_prediction(predictions, targets, dates)

    # ì‹œë®¬ë ˆì´ì…˜ ì˜ˆì‹œ
    sample_vol = predictions[0]
    base_price = test_df.loc[dates[0], 'Coffee_Price'] if dates[0] in test_df.index else 200
    sim_prices = simulate_price_curve(dates[0], sample_vol, base_price)
    sim_prices.plot(title=f"Simulated Coffee Price (start={dates[0].date()}, vol={sample_vol:.4f})", figsize=(12, 6))


def check_current_code_leakage():
    """í˜„ì¬ ì½”ë“œì—ì„œ ë°œìƒí•˜ëŠ” ë°ì´í„° ëˆ„ì¶œ í™•ì¸"""
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í˜„ì¬ ì½”ë“œ ì‹œë®¬ë ˆì´ì…˜)
    dates = pd.date_range('2020-01-01', periods=100, freq='D')
    df = pd.DataFrame({
        'Date': dates,
        'Coffee_Price': 200 + np.cumsum(np.random.randn(100) * 0.02),
        'Coffee_Price_Return': np.random.randn(100) * 0.02
    })
    df.set_index('Date', inplace=True)
    
    # í˜„ì¬ feature_engineering.pyì™€ ë™ì¼í•œ ë°©ì‹
    df['log_return'] = np.log(df['Coffee_Price']) - np.log(df['Coffee_Price'].shift(1))
    df['target_volatility_14d'] = df['log_return'].rolling(14).std().shift(-13)
    
    # í˜„ì¬ main.pyì™€ ë™ì¼í•œ ë°©ì‹
    y_col = 'target_volatility_14d'
    
    print("=== í˜„ì¬ ì½”ë“œì˜ ë°ì´í„° ëˆ„ì¶œ ë¬¸ì œ ===")
    print(f"ì „ì²´ ì»¬ëŸ¼: {list(df.columns)}")
    print(f"íƒ€ê²Ÿ ì»¬ëŸ¼: {y_col}")
    
    # í˜„ì¬ ë°©ì‹ëŒ€ë¡œ X, y ìƒì„±
    X_current = df.values  # ëª¨ë“  ì»¬ëŸ¼ í¬í•¨ (target_volatility_14dë„ í¬í•¨!)
    y_current = df[y_col].values
    
    print(f"\nXì˜ shape: {X_current.shape}")
    print(f"yì˜ shape: {y_current.shape}")
    
    # target_volatility_14dê°€ Xì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    target_col_idx = df.columns.get_loc(y_col)
    print(f"\nğŸš¨ ë°ì´í„° ëˆ„ì¶œ í™•ì¸:")
    print(f"target_volatility_14dê°€ Xì˜ {target_col_idx}ë²ˆì§¸ ì»¬ëŸ¼ì— í¬í•¨ë¨!")
    print(f"X[:5, {target_col_idx}] = {X_current[:5, target_col_idx]}")
    print(f"y[:5] = {y_current[:5]}")
    print(f"ê°’ì´ ë™ì¼í•œê°€? {np.array_equal(X_current[:, target_col_idx], y_current)}")
    
    return df, X_current, y_current, target_col_idx

def fix_data_leakage():
    """ë°ì´í„° ëˆ„ì¶œ ë¬¸ì œ ìˆ˜ì •"""
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    dates = pd.date_range('2020-01-01', periods=100, freq='D')
    df = pd.DataFrame({
        'Date': dates,
        'Coffee_Price': 200 + np.cumsum(np.random.randn(100) * 0.02),
        'Coffee_Price_Return': np.random.randn(100) * 0.02,
        'USD_BRL': 5.0 + np.cumsum(np.random.randn(100) * 0.01),
        'Crude_Oil_Price': 70 + np.cumsum(np.random.randn(100) * 0.5)
    })
    df.set_index('Date', inplace=True)
    
    # ì˜¬ë°”ë¥¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    df['log_return'] = np.log(df['Coffee_Price']) - np.log(df['Coffee_Price'].shift(1))
    
    # í”¼ì²˜ë“¤ (ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©)
    df['volatility_5d'] = df['log_return'].rolling(5).std()
    df['volatility_10d'] = df['log_return'].rolling(10).std()
    df['momentum_5d'] = df['Coffee_Price'] - df['Coffee_Price'].shift(5)
    
    # íƒ€ê²Ÿ (ë¯¸ë˜ ë°ì´í„° ì‚¬ìš© - ì •ìƒ)
    df['target_volatility_14d'] = df['log_return'].rolling(14).std().shift(-13)
    
    print("\n=== ìˆ˜ì •ëœ ì½”ë“œ ===")
    print(f"ì „ì²´ ì»¬ëŸ¼: {list(df.columns)}")
    
    # ì˜¬ë°”ë¥¸ ë°©ì‹: íƒ€ê²Ÿ ì»¬ëŸ¼ì„ Xì—ì„œ ì œì™¸
    feature_cols = [col for col in df.columns if not col.startswith('target_')]
    target_col = 'target_volatility_14d'
    
    print(f"í”¼ì²˜ ì»¬ëŸ¼ë“¤: {feature_cols}")
    print(f"íƒ€ê²Ÿ ì»¬ëŸ¼: {target_col}")
    
    # Xì—ëŠ” í”¼ì²˜ë§Œ, yì—ëŠ” íƒ€ê²Ÿë§Œ
    X_fixed = df[feature_cols].values
    y_fixed = df[target_col].values
    
    print(f"\nìˆ˜ì • í›„:")
    print(f"Xì˜ shape: {X_fixed.shape}")
    print(f"yì˜ shape: {y_fixed.shape}")
    print(f"âœ… íƒ€ê²Ÿ ë³€ìˆ˜ê°€ Xì— í¬í•¨ë˜ì§€ ì•ŠìŒ!")
    
    return df, X_fixed, y_fixed, feature_cols

def demonstrate_impact():
    """ë°ì´í„° ëˆ„ì¶œì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì‹œì—°"""
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error
    from sklearn.model_selection import train_test_split
    
    # ìƒ˜í”Œ ë°ì´í„°
    np.random.seed(42)
    n_samples = 1000
    X_normal = np.random.randn(n_samples, 5)  # ì •ìƒì ì¸ í”¼ì²˜ë“¤
    y = np.sum(X_normal, axis=1) + np.random.randn(n_samples) * 0.1  # íƒ€ê²Ÿ
    
    # ë°ì´í„° ëˆ„ì¶œ ì¼€ì´ìŠ¤: Xì— yê°’ ì¶”ê°€
    X_leaked = np.column_stack([X_normal, y + np.random.randn(n_samples) * 0.01])
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_normal_train, X_normal_test, y_train, y_test = train_test_split(
        X_normal, y, test_size=0.2, random_state=42)
    X_leaked_train, X_leaked_test, _, _ = train_test_split(
        X_leaked, y, test_size=0.2, random_state=42)
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    rf_normal = RandomForestRegressor(random_state=42)
    rf_leaked = RandomForestRegressor(random_state=42)
    
    rf_normal.fit(X_normal_train, y_train)
    rf_leaked.fit(X_leaked_train, y_train)
    
    pred_normal = rf_normal.predict(X_normal_test)
    pred_leaked = rf_leaked.predict(X_leaked_test)
    
    mse_normal = mean_squared_error(y_test, pred_normal)
    mse_leaked = mean_squared_error(y_test, pred_leaked)
    
    print(f"\n=== ë°ì´í„° ëˆ„ì¶œ ì˜í–¥ ì‹œì—° ===")
    print(f"ì •ìƒì ì¸ ëª¨ë¸ MSE: {mse_normal:.6f}")
    print(f"ë°ì´í„° ëˆ„ì¶œ ëª¨ë¸ MSE: {mse_leaked:.6f}")
    print(f"ì„±ëŠ¥ ê°œì„ : {((mse_normal - mse_leaked) / mse_normal * 100):.1f}%")
    print(f"ğŸš¨ ë°ì´í„° ëˆ„ì¶œë¡œ ì¸í•œ ê³¼ë„í•œ ì„±ëŠ¥ í–¥ìƒì€ ì‹¤ì œ ìš´ìš©ì—ì„œ ì¬í˜„ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")

if __name__ == '__main__':
    main()

    # 1. í˜„ì¬ ì½”ë“œì˜ ë¬¸ì œì  í™•ì¸
    df_current, X_current, y_current, target_idx = check_current_code_leakage()
    
    # 2. ìˆ˜ì •ëœ ì½”ë“œ
    df_fixed, X_fixed, y_fixed, feature_cols = fix_data_leakage()
    
    # 3. ë°ì´í„° ëˆ„ì¶œ ì˜í–¥ ì‹œì—°
    demonstrate_impact()