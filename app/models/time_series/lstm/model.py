import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
import os
import warnings
warnings.filterwarnings('ignore')

# Entmax15 êµ¬í˜„ (Softmax ëŒ€ì²´)
class Entmax15(nn.Module):
    def __init__(self, dim=1):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        return entmax15(x, self.dim)

def entmax15(x, dim=1):
    """
    Entmax 1.5 í•¨ìˆ˜: Softmaxì˜ ìŠ¤íŒŒìŠ¤ ëŒ€ì•ˆìœ¼ë¡œ, ì¤‘ìš”ë„ê°€ ë‚®ì€ ì…ë ¥ì— 0 ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹
    """
    # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ì…ë ¥ì—ì„œ ìµœëŒ€ê°’ ì œê±°
    x = x - x.max(dim=dim, keepdim=True)[0]
    
    # entmax15 ì•Œê³ ë¦¬ì¦˜
    tau_star = self_supporting_threshold_entmax15(x, dim)
    y = torch.clamp(x - tau_star, min=0) ** 0.5
    return y / y.sum(dim=dim, keepdim=True)

def self_supporting_threshold_entmax15(x, dim=1):
    """Entmax15ë¥¼ ìœ„í•œ threshold ê³„ì‚° (ì•ˆì „í•œ ë²„ì „)"""
    n_features = x.size(dim)
    x_sorted, _ = torch.sort(x, dim=dim, descending=True)
    
    # ëˆ„ì í•© ê³„ì‚°
    csm = torch.cumsum(x_sorted, dim=dim)
    rhos = torch.arange(1, n_features + 1, device=x.device, dtype=x.dtype)
    
    support = rhos * x_sorted - csm + 0.5
    support_thresh = support / rhos
    
    # threshold ê³„ì‚° (ì•ˆì „í•˜ê²Œ ì¸ë±ìŠ¤ êµ¬í•˜ê¸°)
    support_size = torch.sum(support > 0, dim=dim, keepdim=True).clamp(min=1)  # ìµœì†Œ 1 ë³´ì¥
    
    # ì•ˆì „í•œ ì¸ë±ìŠ¤ ê³„ì‚°
    support_size = torch.min(support_size, torch.tensor([n_features - 1], device=support_size.device))
    
    # torch.gather ì‚¬ìš© ì‹œ ì¸ë±ìŠ¤ í™•ì¸
    gather_indices = support_size.long()
    
    try:
        tau = torch.gather(support_thresh, dim, gather_indices)
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ fallback: ë‹¨ìˆœ í‰ê·  ì‚¬ìš©
        print(f"âš ï¸ Entmax ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. Softmaxë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return torch.zeros_like(x)  # Softmaxì™€ ìœ ì‚¬í•œ íš¨ê³¼ë¥¼ ìœ„í•´ ìƒìˆ˜ ë°˜í™˜
    
    return tau

# ì‹œê³„ì—´ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# LSTM + Softmax Attention ëª¨ë¸ ì •ì˜ (Entmax ëŒ€ì‹  Softmax ì‚¬ìš©)
class LSTMAttentionSoftmax(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.2):
        super(LSTMAttentionSoftmax, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_dim, 
            hidden_dim, 
            num_layers=num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Attention ë ˆì´ì–´
        self.W_a = nn.Linear(hidden_dim, hidden_dim)
        self.v_a = nn.Linear(hidden_dim, 1)
        
        # Softmax í™œì„±í™” í•¨ìˆ˜
        self.softmax = nn.Softmax(dim=1)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.fc = nn.Linear(hidden_dim, output_dim)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        
        # LSTM í†µê³¼
        lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, hidden_dim)
        
        # Attention ê³„ì‚°
        att_energies = self.v_a(torch.tanh(self.W_a(lstm_out)))  # (batch_size, seq_len, 1)
        att_energies = att_energies.squeeze(-1)  # (batch_size, seq_len)
        
        # Softmaxë¡œ attention weights ê³„ì‚° (Entmax ëŒ€ì‹ )
        att_weights = self.softmax(att_energies)  # (batch_size, seq_len)
        
        # Context vector ê³„ì‚°
        context = torch.bmm(att_weights.unsqueeze(1), lstm_out)  # (batch_size, 1, hidden_dim)
        context = context.squeeze(1)  # (batch_size, hidden_dim)
        
        # ë“œë¡­ì•„ì›ƒ ì ìš©
        context = self.dropout(context)
        
        # ìµœì¢… ì˜ˆì¸¡
        output = self.fc(context)  # (batch_size, output_dim)
        
        return output, att_weights

# ìˆ˜ì •ëœ LSTM + Attention + Entmax ëª¨ë¸ ì •ì˜ (ì•ˆì „í•œ ë²„ì „)
class LSTMAttentionEntmax(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.2, use_entmax=True):
        super(LSTMAttentionEntmax, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.use_entmax = use_entmax
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_dim, 
            hidden_dim, 
            num_layers=num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Attention ë ˆì´ì–´
        self.W_a = nn.Linear(hidden_dim, hidden_dim)
        self.v_a = nn.Linear(hidden_dim, 1)
        
        # Attention í™œì„±í™” í•¨ìˆ˜ (Entmax15 ë˜ëŠ” Softmax)
        if use_entmax:
            self.attention_fn = Entmax15(dim=1)
        else:
            self.attention_fn = nn.Softmax(dim=1)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.fc = nn.Linear(hidden_dim, output_dim)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        batch_size, seq_len, _ = x.size()
        
        try:
            # LSTM í†µê³¼
            lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, hidden_dim)
            
            # Attention ê³„ì‚°
            att_energies = self.v_a(torch.tanh(self.W_a(lstm_out)))  # (batch_size, seq_len, 1)
            att_energies = att_energies.squeeze(-1)  # (batch_size, seq_len)
            
            # Attention weights ê³„ì‚° (Entmax ë˜ëŠ” Softmax)
            try:
                att_weights = self.attention_fn(att_energies)  # (batch_size, seq_len)
            except Exception as e:
                print(f"[ê²½ê³ ] Attention ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. Softmaxë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                # Fallback: Softmax ì‚¬ìš©
                att_weights = F.softmax(att_energies, dim=1)
            
            # Context vector ê³„ì‚°
            context = torch.bmm(att_weights.unsqueeze(1), lstm_out)  # (batch_size, 1, hidden_dim)
            context = context.squeeze(1)  # (batch_size, hidden_dim)
            
            # ë“œë¡­ì•„ì›ƒ ì ìš©
            context = self.dropout(context)
            
            # ìµœì¢… ì˜ˆì¸¡
            output = self.fc(context)  # (batch_size, output_dim)
            
            return output, att_weights
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ìˆœì „íŒŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            print("âš ï¸ ê¸´ê¸‰ ë³µêµ¬: í‰ê·  í’€ë§ì„ ì‚¬ìš©í•œ ë‹¨ìˆœ ì˜ˆì¸¡ ìˆ˜í–‰")
            
            # ê¸´ê¸‰ ë³µêµ¬: í‰ê·  í’€ë§ í›„ ì˜ˆì¸¡
            avg_pooled = torch.mean(x, dim=1)  # (batch_size, input_dim)
            
            # ë‹¨ìˆœ ì˜ˆì¸¡ì„ ìœ„í•œ ì„ì‹œ ë ˆì´ì–´
            if not hasattr(self, 'emergency_fc'):
                self.emergency_fc = nn.Linear(x.size(2), self.fc.out_features).to(x.device)
            
            # ì„ì‹œ ì˜ˆì¸¡
            emergency_output = self.emergency_fc(avg_pooled)  # (batch_size, output_dim)
            
            # ê°€ì§œ attention weights ìƒì„±
            fake_weights = torch.ones(batch_size, seq_len).to(x.device) / seq_len
            
            return emergency_output, fake_weights

# Huber Loss í´ë˜ìŠ¤ ì •ì˜
class HuberLoss(nn.Module):
    def __init__(self, delta=1.0):
        super(HuberLoss, self).__init__()
        self.delta = delta

    def forward(self, y_pred, y_true):
        abs_error = torch.abs(y_pred - y_true)
        quadratic = torch.min(abs_error, torch.tensor(self.delta).to(y_pred.device))
        linear = abs_error - quadratic
        loss = 0.5 * quadratic.pow(2) + self.delta * linear
        return loss.mean()

# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_data(df, price_col='Coffee_Price', return_col='Coffee_Price_Return'):
    """ë°ì´í„° ì „ì²˜ë¦¬: ì •ê·œí™” ë° íŠ¹ì„± ì¶”ê°€"""
    # ë‚ ì§œ ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
    df['Date'] = pd.to_datetime(df['Date'])
    df.set_index('Date', inplace=True)
    
    # ì¶”ê°€ ë³€ë™ì„± ê´€ë ¨ íŒŒìƒ í”¼ì²˜ ìƒì„±
    # 1. ì ˆëŒ€ ìˆ˜ìµë¥ 
    df['Abs_Return'] = np.abs(df[return_col])
    
    # 2. nì¼ ë³€ë™ì„±
    for n in [5, 10, 20]:
        df[f'Volatility_{n}d'] = df[return_col].rolling(window=n).std()
    
    # 3. ëª¨ë©˜í…€ (nì¼ ì „ ëŒ€ë¹„ ê°€ê²© ë³€í™”)
    for n in [5, 10, 20]:
        df[f'Momentum_{n}d'] = df[price_col] / df[price_col].shift(n) - 1
    
    # 4. ë³¼ë¦°ì € ë°´ë“œ ë„ˆë¹„
    for n in [20]:
        rolling_mean = df[price_col].rolling(window=n).mean()
        rolling_std = df[price_col].rolling(window=n).std()
        df[f'BB_Width_{n}d'] = (rolling_mean + 2*rolling_std - (rolling_mean - 2*rolling_std)) / rolling_mean
    
    # 5. Z-score (í˜„ì¬ ê°€ê²©ì´ ê³¼ê±° nì¼ í‰ê· ì—ì„œ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€)
    for n in [20]:
        rolling_mean = df[price_col].rolling(window=n).mean()
        rolling_std = df[price_col].rolling(window=n).std()
        df[f'Z_Score_{n}d'] = (df[price_col] - rolling_mean) / rolling_std
    
    # NaN ê°’ ì œê±°
    df.dropna(inplace=True)
    
    return df

# ì‹œê³„ì—´ ë°ì´í„° ìœˆë„ìš° ìƒì„± í•¨ìˆ˜
def create_sequences(data, seq_length, pred_length):
    """ì‹œê³„ì—´ ë°ì´í„° ìœˆë„ìš° ìƒì„±: seq_lengthì¼ì˜ ë°ì´í„°ë¡œ pred_lengthì¼ ì˜ˆì¸¡"""
    xs, ys = [], []
    for i in range(len(data) - seq_length - pred_length + 1):
        x = data[i:(i + seq_length)]
        y = data[(i + seq_length):(i + seq_length + pred_length), 0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼(ê°€ê²©)ë§Œ ì˜ˆì¸¡
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜ - loss_fn_type íŒŒë¼ë¯¸í„° ì¶”ê°€
def train_model(model, train_loader, val_loader, epochs, lr=0.001, device='cpu', loss_fn_type='mse', delta=1.0):
    """ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜"""
    print(f"â³ ëª¨ë¸ í›ˆë ¨ ì‹œì‘... (ì†ì‹¤ í•¨ìˆ˜: {loss_fn_type}, ì—í­: {epochs})")

    model.to(device)
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (MSE ë˜ëŠ” Huber)
    if loss_fn_type.lower() == 'huber':
        criterion = HuberLoss(delta=delta)
        print(f"âœ… Huber Loss ì‚¬ìš© (delta={delta})")
    else:
        criterion = nn.MSELoss()
        print(f"âœ… MSE Loss ì‚¬ìš©")
    
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        # í›ˆë ¨ ëª¨ë“œ
        model.train()
        train_loss = 0.0
        batch_count = 0
        
        for X_batch, y_batch in train_loader:
            try:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
                optimizer.zero_grad()
                
                # ìˆœì „íŒŒ
                y_pred, _ = model(X_batch)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = criterion(y_pred, y_batch)
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                optimizer.step()
                
                train_loss += loss.item()
                batch_count += 1
            except Exception as e:
                print(f"âŒ í›ˆë ¨ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        if batch_count > 0:
            train_loss /= batch_count
        
        # ê²€ì¦ ëª¨ë“œ
        model.eval()
        val_loss = 0.0
        batch_count = 0
        
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                try:
                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                    
                    # ìˆœì „íŒŒ
                    y_pred, _ = model(X_batch)
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(y_pred, y_batch)
                    
                    val_loss += loss.item()
                    batch_count += 1
                except Exception as e:
                    print(f"âŒ ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        if batch_count > 0:
            val_loss /= batch_count
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')
    
    print(f"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {epochs}ì—í­")
    return train_losses, val_losses

# ì˜ˆì¸¡ ë° í‰ê°€ í•¨ìˆ˜
def predict_and_evaluate(model, test_loader, scaler, device='cpu'):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ë° í‰ê°€"""
    model.eval()
    predictions = []
    actuals = []
    attention_weights = []
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            try:
                X_batch = X_batch.to(device)
                
                # ì˜ˆì¸¡
                y_pred, att_weights = model(X_batch)
                
                # ê²°ê³¼ ì €ì¥
                predictions.append(y_pred.cpu().numpy())
                actuals.append(y_batch.numpy())
                attention_weights.append(att_weights.cpu().numpy())
            except Exception as e:
                print(f"âŒ ì˜ˆì¸¡ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
    
    if not predictions:
        raise ValueError("âŒ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ë°°ì¹˜ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    # ê²°í•©
    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)
    attention_weights = np.vstack(attention_weights)
    
    # ì—­ì •ê·œí™” (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì¸ ê°€ê²©ë§Œ)
    try:
        # íŒ¨ë”© í¬ê¸° ê³„ì‚°
        pad_width = scaler.scale_.shape[0] - predictions.shape[1]
        
        # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ì„ íŒ¨ë”©í•˜ì—¬ ì—­ì •ê·œí™”
        padded_predictions = np.pad(predictions, ((0, 0), (0, pad_width)), 'constant')
        padded_actuals = np.pad(actuals, ((0, 0), (0, pad_width)), 'constant')
        
        # ì—­ì •ê·œí™”
        predictions_rescaled = scaler.inverse_transform(padded_predictions)[..., 0]
        actuals_rescaled = scaler.inverse_transform(padded_actuals)[..., 0]
    except Exception as e:
        print(f"âš ï¸ ì—­ì •ê·œí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì›ë³¸ ê°’ ì‚¬ìš©.")
        predictions_rescaled = predictions[:, 0]  # ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
        actuals_rescaled = actuals[:, 0]
    
    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    mae = np.mean(np.abs(predictions_rescaled - actuals_rescaled))
    rmse = np.sqrt(np.mean((predictions_rescaled - actuals_rescaled)**2))
    
    print(f'í‰ê°€ ì§€í‘œ - MAE: {mae:.4f}, RMSE: {rmse:.4f}')
    
    return predictions_rescaled, actuals_rescaled, attention_weights, mae, rmse

# ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì˜ˆì¸¡ ë°ì´í„° ìƒì„± í•¨ìˆ˜
def sliding_window_prediction(model, data, scaler, seq_length, pred_length, stride=1, device='cpu'):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡"""
    model.eval()
    
    # ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ ì¤€ë¹„
    all_predictions = []
    
    for i in range(0, len(data) - seq_length - pred_length + 1, stride):
        try:
            # ì‹œí€€ìŠ¤ ë°ì´í„° ì¶”ì¶œ
            sequence = data[i:i+seq_length]
            actual = data[i+seq_length:i+seq_length+pred_length, 0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼(ê°€ê²©)
            
            # í…ì„œë¡œ ë³€í™˜
            sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)  # [1, seq_length, n_features]
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                prediction, _ = model(sequence_tensor)
                prediction = prediction.cpu().numpy()

            # ì—­ì •ê·œí™”
            try:
                # ì˜ˆì¸¡ ê²°ê³¼ shape: (1, pred_length)
                prediction = prediction[0]  # shape: (pred_length,)

                # ì‹¤ì œ ê°’ë„ ë™ì¼í•˜ê²Œ
                actual = actual.reshape(-1)  # shape: (pred_length,)

                # scalerë¡œ inverse_transform í•˜ê¸° ìœ„í•´ 2D ë°°ì—´ë¡œ ë§ì¶¤
                if scaler.scale_.shape[0] == 1:
                    # ë‹¨ì¼ í”¼ì²˜ë§Œ ì •ê·œí™”í•œ ê²½ìš°
                    prediction_rescaled = scaler.inverse_transform(prediction.reshape(-1, 1)).flatten()
                    actual_rescaled = scaler.inverse_transform(actual.reshape(-1, 1)).flatten()
                else:
                    # ë‹¤ë³€ëŸ‰ ì •ê·œí™”ëœ ê²½ìš°, ì²« ë²ˆì§¸ í”¼ì²˜(ê°€ê²©)ë§Œ ë³µì›
                    n_features = scaler.scale_.shape[0]

                    # predictionì„ ê°€ê²©ê°’ë§Œ í¬í•¨í•œ ì „ì²´ í”¼ì²˜ ë²¡í„°ë¡œ íŒ¨ë”©
                    pred_padded = np.zeros((pred_length, n_features))
                    pred_padded[:, 0] = prediction  # ê°€ê²©ì´ ì²« ë²ˆì§¸ í”¼ì²˜

                    actual_padded = np.zeros((pred_length, n_features))
                    actual_padded[:, 0] = actual

                    # ì—­ì •ê·œí™”
                    prediction_rescaled = scaler.inverse_transform(pred_padded)[:, 0]
                    actual_rescaled = scaler.inverse_transform(actual_padded)[:, 0]
            except Exception as e:
                print(f"âš ï¸ ì—­ì •ê·œí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì›ë³¸ ê°’ ì‚¬ìš©.")
                prediction_rescaled = prediction
                actual_rescaled = actual

            
            # ì˜ˆì¸¡ ì €ì¥
            all_predictions.append({
                'start_idx': i,
                'end_idx': i + seq_length + pred_length,
                'prediction': prediction_rescaled,
                'actual': actual_rescaled
            })
        except Exception as e:
            print(f"[ì—ëŸ¬] ìœˆë„ìš° {i} ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    return all_predictions

def setup_model(input_dim, use_entmax=False):
    """ëª¨ë¸ ì„¤ì •"""
    print(f"â³ ëª¨ë¸ ì„¤ì • ì¤‘...")
    
    hidden_dim = 128
    output_dim = 14  # 14ì¼ ì˜ˆì¸¡
    num_layers = 2
    dropout = 0.2
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}")
    
    # Entmax ë˜ëŠ” Softmax ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
    if use_entmax:
        print("ğŸ” Entmax Attention ëª¨ë¸ ì‚¬ìš©")
        model = LSTMAttentionEntmax(input_dim, hidden_dim, output_dim, num_layers, dropout, use_entmax=True)
    else:
        print("ğŸ” Softmax Attention ëª¨ë¸ ì‚¬ìš©")
        model = LSTMAttentionSoftmax(input_dim, hidden_dim, output_dim, num_layers, dropout)
    
    print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ: {model.__class__.__name__}")
    
    return model, device

def run_sliding_window_prediction(model, test_data, scaler, seq_length, pred_length, device):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ ì§„í–‰"""
    print(f"â³ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
    
    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì˜ˆì¸¡
    sliding_predictions = sliding_window_prediction(model, test_data, scaler, seq_length, pred_length, stride=1, device=device)
    
    if not sliding_predictions:
        print("âš ï¸ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"âœ… ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì˜ˆì¸¡ ì™„ë£Œ: {len(sliding_predictions)}ê°œ ì˜ˆì¸¡")
    return sliding_predictions


def online_update_prediction(model, test_data, scaler, seq_length, pred_length, device='cpu', lr=1e-4, loss_fn='mse'):
    model.to(device)
    model.train()

    # ì†ì‹¤ í•¨ìˆ˜
    if loss_fn == 'huber':
        criterion = HuberLoss()
    else:
        criterion = nn.MSELoss()

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    predictions = []
    actuals = []

    for i in range(0, len(test_data) - seq_length - pred_length + 1):
        # ì‹œí€€ìŠ¤ ì¤€ë¹„
        input_seq = test_data[i:i+seq_length]
        target_seq = test_data[i+seq_length:i+seq_length+pred_length, 0]  # ì‹¤ì œ ê°€ê²©

        # í…ì„œ ë³€í™˜
        input_tensor = torch.FloatTensor(input_seq).unsqueeze(0).to(device)
        target_tensor = torch.FloatTensor(target_seq).unsqueeze(0).to(device)

        # ì˜ˆì¸¡
        model.eval()
        with torch.no_grad():
            pred_tensor, _ = model(input_tensor)
        model.train()

        # ì˜ˆì¸¡ ì €ì¥
        pred_np = pred_tensor.cpu().numpy().flatten()
        target_np = target_tensor.cpu().numpy().flatten()
        predictions.append(pred_np)
        actuals.append(target_np)

        # ì˜¨ë¼ì¸ í•™ìŠµ (í•œ step)
        optimizer.zero_grad()
        output, _ = model(input_tensor)
        loss = criterion(output, target_tensor)
        loss.backward()
        optimizer.step()

    predictions = np.array(predictions)
    actuals = np.array(actuals)
    return predictions, actuals
