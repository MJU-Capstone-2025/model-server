import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
import os
import warnings
import datetime
from .utils import get_result_dir, save_predictions_to_csv
from .market_calendar import adjust_forecast_for_market_calendar, is_trading_day

warnings.filterwarnings('ignore')

# Entmax15 êµ¬í˜„ (Softmax ëŒ€ì²´)
class Entmax15(nn.Module):
    def __init__(self, dim=1):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        return entmax15(x, self.dim)

def entmax15(x, dim=1):
    """
    Entmax 1.5 í•¨ìˆ˜: Softmaxì˜ ìŠ¤íŒŒìŠ¤ ëŒ€ì•ˆìœ¼ë¡œ, ì¤‘ìš”ë„ê°€ ë‚®ì€ ì…ë ¥ì— 0 ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹
    """
    # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ì…ë ¥ì—ì„œ ìµœëŒ€ê°’ ì œê±°
    x = x - x.max(dim=dim, keepdim=True)[0]
    
    # entmax15 ì•Œê³ ë¦¬ì¦˜
    tau_star = self_supporting_threshold_entmax15(x, dim)
    y = torch.clamp(x - tau_star, min=0) ** 0.5
    return y / y.sum(dim=dim, keepdim=True)

def self_supporting_threshold_entmax15(x, dim=1):
    """Entmax15ë¥¼ ìœ„í•œ threshold ê³„ì‚° (ì•ˆì „í•œ ë²„ì „)"""
    n_features = x.size(dim)
    x_sorted, _ = torch.sort(x, dim=dim, descending=True)
    
    # ëˆ„ì í•© ê³„ì‚°
    csm = torch.cumsum(x_sorted, dim=dim)
    rhos = torch.arange(1, n_features + 1, device=x.device, dtype=x.dtype)
    
    support = rhos * x_sorted - csm + 0.5
    support_thresh = support / rhos
    
    # threshold ê³„ì‚° (ì•ˆì „í•˜ê²Œ ì¸ë±ìŠ¤ êµ¬í•˜ê¸°)
    support_size = torch.sum(support > 0, dim=dim, keepdim=True).clamp(min=1)  # ìµœì†Œ 1 ë³´ì¥
    
    # ì•ˆì „í•œ ì¸ë±ìŠ¤ ê³„ì‚°
    support_size = torch.min(support_size, torch.tensor([n_features - 1], device=support_size.device))
    
    # torch.gather ì‚¬ìš© ì‹œ ì¸ë±ìŠ¤ í™•ì¸
    gather_indices = support_size.long()
    
    try:
        tau = torch.gather(support_thresh, dim, gather_indices)
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ fallback: ë‹¨ìˆœ í‰ê·  ì‚¬ìš©
        print(f"âš ï¸ Entmax ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. Softmaxë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return torch.zeros_like(x)  # Softmaxì™€ ìœ ì‚¬í•œ íš¨ê³¼ë¥¼ ìœ„í•´ ìƒìˆ˜ ë°˜í™˜
    
    return tau

# ì‹œê³„ì—´ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# LSTM + Softmax Attention ëª¨ë¸ ì •ì˜ (Entmax ëŒ€ì‹  Softmax ì‚¬ìš©)
class LSTMAttentionSoftmax(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.2):
        super(LSTMAttentionSoftmax, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_dim, 
            hidden_dim, 
            num_layers=num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Attention ë ˆì´ì–´
        self.W_a = nn.Linear(hidden_dim, hidden_dim)
        self.v_a = nn.Linear(hidden_dim, 1)
        
        # Softmax í™œì„±í™” í•¨ìˆ˜
        self.softmax = nn.Softmax(dim=1)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.fc = nn.Linear(hidden_dim, output_dim)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        
        # LSTM í†µê³¼
        lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, hidden_dim)
        
        # Attention ê³„ì‚°
        att_energies = self.v_a(torch.tanh(self.W_a(lstm_out)))  # (batch_size, seq_len, 1)
        att_energies = att_energies.squeeze(-1)  # (batch_size, seq_len)
        
        # Softmaxë¡œ attention weights ê³„ì‚° (Entmax ëŒ€ì‹ )
        att_weights = self.softmax(att_energies)  # (batch_size, seq_len)
        
        # Context vector ê³„ì‚°
        context = torch.bmm(att_weights.unsqueeze(1), lstm_out)  # (batch_size, 1, hidden_dim)
        context = context.squeeze(1)  # (batch_size, hidden_dim)
        
        # ë“œë¡­ì•„ì›ƒ ì ìš©
        context = self.dropout(context)
        
        # ìµœì¢… ì˜ˆì¸¡
        output = self.fc(context)  # (batch_size, output_dim)
        
        return output, att_weights

# LSTM + Attention + Entmax ëª¨ë¸ ì •ì˜ -> ì•ˆì”€ 
class LSTMAttentionEntmax(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.2, use_entmax=True):
        super(LSTMAttentionEntmax, self).__init__()
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.use_entmax = use_entmax
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_dim, 
            hidden_dim, 
            num_layers=num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Attention ë ˆì´ì–´
        self.W_a = nn.Linear(hidden_dim, hidden_dim)
        self.v_a = nn.Linear(hidden_dim, 1)
        
        # Attention í™œì„±í™” í•¨ìˆ˜ (Entmax15 ë˜ëŠ” Softmax)
        if use_entmax:
            self.attention_fn = Entmax15(dim=1)
        else:
            self.attention_fn = nn.Softmax(dim=1)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.fc = nn.Linear(hidden_dim, output_dim)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        batch_size, seq_len, _ = x.size()
        
        try:
            # LSTM í†µê³¼
            lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, hidden_dim)
            
            # Attention ê³„ì‚°
            att_energies = self.v_a(torch.tanh(self.W_a(lstm_out)))  # (batch_size, seq_len, 1)
            att_energies = att_energies.squeeze(-1)  # (batch_size, seq_len)
            
            # Attention weights ê³„ì‚° (Entmax ë˜ëŠ” Softmax)
            try:
                att_weights = self.attention_fn(att_energies)  # (batch_size, seq_len)
            except Exception as e:
                print(f"âš ï¸ Attention ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. Softmaxë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                # Fallback: Softmax ì‚¬ìš©
                att_weights = F.softmax(att_energies, dim=1)
            
            # Context vector ê³„ì‚°
            context = torch.bmm(att_weights.unsqueeze(1), lstm_out)  # (batch_size, 1, hidden_dim)
            context = context.squeeze(1)  # (batch_size, hidden_dim)
            
            # ë“œë¡­ì•„ì›ƒ ì ìš©
            context = self.dropout(context)
            
            # ìµœì¢… ì˜ˆì¸¡
            output = self.fc(context)  # (batch_size, output_dim)
            
            return output, att_weights
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ìˆœì „íŒŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            print("âš ï¸ ê¸´ê¸‰ ë³µêµ¬: í‰ê·  í’€ë§ì„ ì‚¬ìš©í•œ ë‹¨ìˆœ ì˜ˆì¸¡ ìˆ˜í–‰")
            
            # ê¸´ê¸‰ ë³µêµ¬: í‰ê·  í’€ë§ í›„ ì˜ˆì¸¡
            avg_pooled = torch.mean(x, dim=1)  # (batch_size, input_dim)
            
            # ë‹¨ìˆœ ì˜ˆì¸¡ì„ ìœ„í•œ ì„ì‹œ ë ˆì´ì–´
            if not hasattr(self, 'emergency_fc'):
                self.emergency_fc = nn.Linear(x.size(2), self.fc.out_features).to(x.device)
            
            # ì„ì‹œ ì˜ˆì¸¡
            emergency_output = self.emergency_fc(avg_pooled)  # (batch_size, output_dim)
            
            # ê°€ì§œ attention weights ìƒì„±
            fake_weights = torch.ones(batch_size, seq_len).to(x.device) / seq_len
            
            return emergency_output, fake_weights

# Huber Loss í´ë˜ìŠ¤ ì •ì˜
class HuberLoss(nn.Module):
    def __init__(self, delta=1.0):
        super(HuberLoss, self).__init__()
        self.delta = delta

    def forward(self, y_pred, y_true):
        abs_error = torch.abs(y_pred - y_true)
        quadratic = torch.min(abs_error, torch.tensor(self.delta).to(y_pred.device))
        linear = abs_error - quadratic
        loss = 0.5 * quadratic.pow(2) + self.delta * linear
        return loss.mean()

# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_data(df, price_col='Coffee_Price', return_col='Coffee_Price_Return'):
    """ë°ì´í„° ì „ì²˜ë¦¬: ì •ê·œí™” ë° íŠ¹ì„± ì¶”ê°€"""
    # ë‚ ì§œ ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
    df['Date'] = pd.to_datetime(df['Date'])
    df.set_index('Date', inplace=True)
    
    # ì¶”ê°€ ë³€ë™ì„± ê´€ë ¨ íŒŒìƒ í”¼ì²˜ ìƒì„±
    # 1. ì ˆëŒ€ ìˆ˜ìµë¥ 
    df['Abs_Return'] = np.abs(df[return_col])
    
    # 2. nì¼ ë³€ë™ì„±
    for n in [5, 10, 20]:
        df[f'Volatility_{n}d'] = df[return_col].rolling(window=n).std()
    
    # 3. ëª¨ë©˜í…€ (nì¼ ì „ ëŒ€ë¹„ ê°€ê²© ë³€í™”)
    for n in [5, 10, 20]:
        df[f'Momentum_{n}d'] = df[price_col] / df[price_col].shift(n) - 1
    
    # 4. ë³¼ë¦°ì € ë°´ë“œ ë„ˆë¹„
    for n in [20]:
        rolling_mean = df[price_col].rolling(window=n).mean()
        rolling_std = df[price_col].rolling(window=n).std()
        df[f'BB_Width_{n}d'] = (rolling_mean + 2*rolling_std - (rolling_mean - 2*rolling_std)) / rolling_mean
    
    # 5. Z-score (í˜„ì¬ ê°€ê²©ì´ ê³¼ê±° nì¼ í‰ê· ì—ì„œ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€)
    for n in [20]:
        rolling_mean = df[price_col].rolling(window=n).mean()
        rolling_std = df[price_col].rolling(window=n).std()
        df[f'Z_Score_{n}d'] = (df[price_col] - rolling_mean) / rolling_std
    
    # NaN ê°’ ì œê±°
    df.dropna(inplace=True)
    
    return df

# ì‹œê³„ì—´ ë°ì´í„° ìœˆë„ìš° ìƒì„± í•¨ìˆ˜
def create_sequences(data, seq_length, pred_length):
    """ì‹œê³„ì—´ ë°ì´í„° ìœˆë„ìš° ìƒì„±: seq_lengthì¼ì˜ ë°ì´í„°ë¡œ pred_lengthì¼ ì˜ˆì¸¡"""
    xs, ys = [], []
    for i in range(len(data) - seq_length - pred_length + 1):
        x = data[i:(i + seq_length)]
        y = data[(i + seq_length):(i + seq_length + pred_length), 0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼(ê°€ê²©)ë§Œ ì˜ˆì¸¡
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜ - loss_fn_type íŒŒë¼ë¯¸í„° ì¶”ê°€
def train_model(model, train_loader, val_loader, epochs, lr=0.001, device='cpu', loss_fn_type='mse', delta=1.0):
    """ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜"""
    print(f"â³ ëª¨ë¸ í›ˆë ¨ ì‹œì‘... (ì†ì‹¤ í•¨ìˆ˜: {loss_fn_type}, ì—í­: {epochs})")

    model.to(device)
    
    # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (MSE ë˜ëŠ” Huber)
    if loss_fn_type.lower() == 'huber':
        criterion = HuberLoss(delta=delta)
        print(f"âœ… Huber Loss ì‚¬ìš© (delta={delta})")
    else:
        criterion = nn.MSELoss()
        print(f"âœ… MSE Loss ì‚¬ìš©")
    
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    train_losses = []
    val_losses = []
    
    for epoch in range(epochs):
        # í›ˆë ¨ ëª¨ë“œ
        model.train()
        train_loss = 0.0
        batch_count = 0
        
        for X_batch, y_batch in train_loader:
            try:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
                optimizer.zero_grad()
                
                # ìˆœì „íŒŒ
                y_pred, _ = model(X_batch)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = criterion(y_pred, y_batch)
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                optimizer.step()
                
                train_loss += loss.item()
                batch_count += 1
            except Exception as e:
                print(f"âŒ í›ˆë ¨ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        if batch_count > 0:
            train_loss /= batch_count
        
        # ê²€ì¦ ëª¨ë“œ
        model.eval()
        val_loss = 0.0
        batch_count = 0
        
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                try:
                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                    
                    # ìˆœì „íŒŒ
                    y_pred, _ = model(X_batch)
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(y_pred, y_batch)
                    
                    val_loss += loss.item()
                    batch_count += 1
                except Exception as e:
                    print(f"âŒ ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        if batch_count > 0:
            val_loss /= batch_count
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')
    
    print(f"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {epochs}ì—í­")
    return train_losses, val_losses

def returns_to_price(returns, start_price):
    """ìˆ˜ìµë¥  ì‹œí€€ìŠ¤ë¥¼ ëˆ„ì  ê³±í•˜ì—¬ price ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    # returns: (N,) ë˜ëŠ” (batch, N)
    # start_price: float ë˜ëŠ” (batch,)
    if returns.ndim == 1:
        return start_price * np.cumprod(1 + returns)
    elif returns.ndim == 2:
        return np.array([sp * np.cumprod(1 + r) for r, sp in zip(returns, start_price)])
    else:
        raise ValueError('returns shape error')

def predict_and_evaluate(model, test_loader, scaler, device='cpu', test_dates=None, folder_name=None, target='price'):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ë° í‰ê°€ + íœ´ì¥ì¼ ë³´ì • ë° csv ì €ì¥ (target: price/return)"""
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta
    from .market_calendar import adjust_forecast_for_market_calendar, is_trading_day

    model.eval()
    predictions = []
    actuals = []
    attention_weights = []
    all_dates = []

    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            try:
                X_batch = X_batch.to(device)
                y_pred, att_weights = model(X_batch)
                predictions.append(y_pred.cpu().numpy())
                actuals.append(y_batch.numpy())
                attention_weights.append(att_weights.cpu().numpy())
            except Exception as e:
                print(f"âŒ ì˜ˆì¸¡ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue

    if not predictions:
        raise ValueError("âŒ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ë°°ì¹˜ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)
    attention_weights = np.vstack(attention_weights)

    # ì—­ì •ê·œí™” (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì¸ ê°€ê²©/ìˆ˜ìµë¥ ë§Œ)
    try:
        pad_width = scaler.scale_.shape[0] - predictions.shape[1]
        padded_predictions = np.pad(predictions, ((0, 0), (0, pad_width)), 'constant')
        padded_actuals = np.pad(actuals, ((0, 0), (0, pad_width)), 'constant')
        if target == 'price':
            predictions_rescaled = scaler.inverse_transform(padded_predictions)[..., 0]
            actuals_rescaled = scaler.inverse_transform(padded_actuals)[..., 0]
        elif target == 'return':
            # ìˆ˜ìµë¥  ì»¬ëŸ¼ì´ 1ë²ˆì´ë¼ê³  ê°€ì • (Coffee_Price_Return)
            predictions_rescaled = scaler.inverse_transform(padded_predictions)[..., 1]
            actuals_rescaled = scaler.inverse_transform(padded_actuals)[..., 1]
        else:
            raise ValueError(f'Unknown target: {target}')
    except Exception as e:
        print(f"âš ï¸ ì—­ì •ê·œí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì›ë³¸ ê°’ ì‚¬ìš©.")
        predictions_rescaled = predictions[:, 0]
        actuals_rescaled = actuals[:, 0]

    # === price/return ë³€í™˜ ===
    if target == 'return':
        # test_datesê°€ ìˆìœ¼ë©´ ì²«ë‚  ê°€ê²©ì„ test_dates[0]ë¡œë¶€í„° ì¶”ì¶œ
        # ì—†ìœ¼ë©´ 200ìœ¼ë¡œ ì„ì‹œ(ì‚¬ìš©ì ë§ì¶¤ í•„ìš”)
        if test_dates is not None and hasattr(test_dates, '__len__') and len(test_dates) > 0:
            # test_loader.dataset[0][0]ì˜ ë§ˆì§€ë§‰ priceë¥¼ ì“°ëŠ” ê²Œ ë” ì •í™•í•˜ì§€ë§Œ, ì—¬ê¸°ì„  test_dates[0]ì— í•´ë‹¹í•˜ëŠ” priceë¥¼ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •
            # ì‹¤ì œë¡œëŠ” test setì˜ ì²«ë‚  priceë¥¼ ë³„ë„ ì „ë‹¬ë°›ëŠ” ê²Œ ê°€ì¥ ì•ˆì „í•¨
            # ì—¬ê¸°ì„œëŠ” scalerì˜ min/maxë¡œ ì—­ì •ê·œí™”í•œ priceë¥¼ ì‚¬ìš©(0ë²ˆ ì»¬ëŸ¼)
            # padded_predictions[0, 0]ì€ ì²«ë‚  priceì˜ ì •ê·œí™” ê°’
            # ì—­ì •ê·œí™”
            first_price_norm = padded_predictions[0, 0]
            first_price = scaler.inverse_transform([padded_predictions[0]])[0, 0]
        else:
            first_price = 200.0  # ì„ì‹œê°’
        price_pred = returns_to_price(predictions_rescaled, first_price)
        price_actual = returns_to_price(actuals_rescaled, first_price)
    else:
        price_pred = predictions_rescaled
        price_actual = actuals_rescaled

    # ì²«ë‚  ë³´ì • (price ê¸°ì¤€)
    if len(price_pred) > 0 and len(price_actual) > 0:
        shift = price_actual[0] - price_pred[0]
        price_pred = price_pred + shift

    # ì˜ˆì¸¡ êµ¬ê°„ ë‚ ì§œ ìƒì„± (test_datesê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©, ì—†ìœ¼ë©´ ì—°ì† ë‚ ì§œ ìƒì„±)
    if test_dates is not None:
        start_date = pd.to_datetime(test_dates[0])
        end_date = pd.to_datetime(test_dates[0]) + timedelta(days=len(price_pred)-1)
        all_dates = pd.date_range(start=start_date, end=end_date, freq='D')
        if not isinstance(test_dates, list):
            test_dates_str = [d.strftime('%Y-%m-%d') if hasattr(d, 'strftime') else str(d) for d in test_dates]
        else:
            test_dates_str = test_dates
    else:
        all_dates = pd.RangeIndex(len(price_pred))
        test_dates_str = None

    # íœ´ì¥ì¼ ë³´ì •: íœ´ì¥ì¼ì—ëŠ” ê°€ì¥ ìµœê·¼ ê±°ë˜ì¼ì˜ ì˜ˆì¸¡ê°’ì„ ë³µì‚¬
    pred_series = pd.Series(price_pred, index=all_dates)
    adj_pred_series = pred_series.copy()
    for i in range(1, len(all_dates)):
        d = all_dates[i]
        if not is_trading_day(d):
            prev_idx = i-1
            while prev_idx >= 0 and not is_trading_day(all_dates[prev_idx]):
                prev_idx -= 1
            if prev_idx >= 0:
                adj_pred_series[d] = adj_pred_series[all_dates[prev_idx]]
    # ì‹¤ì œê°’ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë§ì¶”ë˜, ì—†ëŠ” ë‚ ì§œëŠ” nan
    adj_actuals = []
    for d in all_dates:
        d_str = d.strftime('%Y-%m-%d') if hasattr(d, 'strftime') else str(d)
        if test_dates_str is not None and d_str in test_dates_str:
            idx = test_dates_str.index(d_str)
            adj_actuals.append(price_actual[idx])
        else:
            adj_actuals.append(np.nan)

    # csv ì €ì¥ (price/return ëª¨ë‘ ì €ì¥)
    all_data = []
    for i, d in enumerate(all_dates):
        data_row = {
            'window_id': 1,
            'date': d.strftime('%Y-%m-%d') if hasattr(d, 'strftime') else str(d),
            'step': i+1,
            'prediction_price': adj_pred_series.iloc[i],
            'actual_price': adj_actuals[i],
            'prediction_return': predictions_rescaled[i] if target == 'return' else np.nan,
            'actual_return': actuals_rescaled[i] if target == 'return' else np.nan,
            'start_idx': 0,
            'end_idx': len(all_dates)
        }
        all_data.append(data_row)
    df = pd.DataFrame(all_data)
    cols = ['window_id', 'date', 'step', 'prediction_price', 'actual_price', 'prediction_return', 'actual_return', 'start_idx', 'end_idx']
    df = df[[col for col in cols if col in df.columns]]
    if folder_name is None:
        folder_name = 'basic_predict'
    result_dir = get_result_dir(folder_name)
    csv_path = os.path.join(result_dir, 'basic_prediction.csv')
    os.makedirs(result_dir, exist_ok=True)
    df.to_csv(csv_path, index=False)
    print(f"âœ… ê¸°ë³¸ ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")

    mae = np.nanmean(np.abs(adj_pred_series.values - np.array(adj_actuals)))
    rmse = np.sqrt(np.nanmean((adj_pred_series.values - np.array(adj_actuals))**2))
    print(f'í‰ê°€ ì§€í‘œ - MAE: {mae:.4f}, RMSE: {rmse:.4f}')

    return adj_pred_series.values, np.array(adj_actuals), attention_weights, mae, rmse

def sliding_window_prediction(model, data, scaler, seq_length, pred_length, stride=14, device='cpu', test_dates=None, folder_name=None, isOnline=False, target='price'):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ (ì˜¨ë¼ì¸ ë˜ëŠ” ì¼ë°˜, target: price/return)"""
    model.eval()
    print(f"[DEBUG] test_dates: {test_dates}")
    if test_dates is not None:
        test_dates = [d.strftime('%Y-%m-%d') if hasattr(d, 'strftime') else str(d) for d in test_dates]
    all_predictions = []

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # ì˜¨ë¼ì¸ í•™ìŠµì„ ìœ„í•œ ì˜µí‹°ë§ˆì´ì € ì„¤ì •

    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡
    for i in range(0, len(data) - seq_length - pred_length + 1, stride):
        try:
            sequence = data[i:i+seq_length].copy()
            # ì‹¤ì œê°’ ì¶”ì¶œ (target ë¶„ê¸°)
            if target == 'price':
                actual = data[i+seq_length:i+seq_length+pred_length, 0].copy().reshape(-1)
            elif target == 'return':
                actual = data[i+seq_length:i+seq_length+pred_length, 1].copy().reshape(-1)
            else:
                raise ValueError(f'Unknown target: {target}')
            sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)

            # ì˜ˆì¸¡
            with torch.no_grad():
                prediction, _ = model(sequence_tensor)
                prediction = prediction.cpu().numpy().reshape(-1)  # (pred_length,)

            # ì—­ì •ê·œí™” (feature ìˆ˜ì— ë§ê²Œ ë”ë¯¸ feature ë¶™ì´ê¸°)
            n_features = scaler.scale_.shape[0]
            pred_padded = np.zeros((pred_length, n_features))
            actual_padded = np.zeros((pred_length, n_features))
            if target == 'price':
                pred_padded[:, 0] = prediction
                actual_padded[:, 0] = actual
                prediction_rescaled = scaler.inverse_transform(pred_padded)[:, 0]
                actual_rescaled = scaler.inverse_transform(actual_padded)[:, 0]
                prediction_return = np.nan * np.ones_like(prediction_rescaled)
                actual_return = np.nan * np.ones_like(actual_rescaled)
                prediction_price = prediction_rescaled
                actual_price = actual_rescaled
            elif target == 'return':
                pred_padded[:, 1] = prediction
                actual_padded[:, 1] = actual
                prediction_rescaled = scaler.inverse_transform(pred_padded)[:, 1]
                actual_rescaled = scaler.inverse_transform(actual_padded)[:, 1]
                # ëˆ„ì  ê³±ìœ¼ë¡œ price í™˜ì‚° (ìœˆë„ìš° ì‹œì‘ì  price í•„ìš”)
                # ìœˆë„ìš° ì‹œì‘ì  priceëŠ” data[i+seq_length-1, 0] (ì •ê·œí™”ëœ ê°’) -> ì—­ì •ê·œí™”
                start_price_norm = data[i+seq_length-1, 0]
                start_price = scaler.inverse_transform([[start_price_norm]+[0]*(n_features-1)])[0, 0]
                prediction_price = returns_to_price(prediction_rescaled, start_price)
                actual_price = returns_to_price(actual_rescaled, start_price)
                prediction_return = prediction_rescaled
                actual_return = actual_rescaled
            else:
                raise ValueError(f'Unknown target: {target}')

            # ë³€ë™ì„± ì™„í™” ë³´ì •: ì˜¨ë¼ì¸ ëª¨ë“œì¼ ë•Œë§Œ ì ìš©
            if isOnline and i > 0:
                previous_prediction = all_predictions[-1]['prediction_price'] if all_predictions else prediction_price
                change = (prediction_price[0] - previous_prediction[0]) / 1 # ë³€í™”ìœ¨ ë³´ì • ì•ˆí•¨
                prediction_price = previous_prediction + change

            # ì˜ˆì¸¡ ê²°ê³¼ ë³´ì •: ì²«ë‚  ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë§Œí¼ ì „ì²´ ì˜ˆì¸¡ê°’ì„ shift (price ê¸°ì¤€)
            shift = actual_price[0] - prediction_price[0]
            prediction_price = prediction_price + shift

            # === íœ´ì¥ì¼ ë³´ì • ===
            # ë‚ ì§œ ì¸ë±ìŠ¤ ìƒì„± (test_datesê°€ ìˆìœ¼ë©´ ê·¸ì— ë§ì¶°ì„œ, ì—†ìœ¼ë©´ None)
            if test_dates is not None:
                # ê° ìœˆë„ìš°ì˜ ì˜ˆì¸¡ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                start_idx = i + seq_length
                end_idx = i + seq_length + pred_length
                window_dates = test_dates[start_idx:end_idx]
                if len(window_dates) == len(prediction_price):
                    pred_series = pd.Series(prediction_price, index=pd.to_datetime(window_dates))
                    prediction_price = adjust_forecast_for_market_calendar(pred_series).values
            # ===

            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (price/return ëª¨ë‘)
            prediction_info = {
                'start_idx': i,
                'end_idx': i + seq_length + pred_length,
                'prediction_price': prediction_price,
                'actual_price': actual_price,
                'prediction_return': prediction_return,
                'actual_return': actual_return,
                'seq_length': seq_length,
                'pred_length': pred_length
            }
            all_predictions.append(prediction_info)

            # ì˜¨ë¼ì¸ í•™ìŠµ (í•œ step): ì˜¨ë¼ì¸ ëª¨ë“œì¼ ë•Œë§Œ ì ìš©
            if isOnline:
                model.train()
                input_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
                target_tensor = torch.FloatTensor(actual).unsqueeze(0).to(device)  # (1, pred_length)
                optimizer.zero_grad()
                output, _ = model(input_tensor)
                output = output.squeeze(0)  # (pred_length,)
                loss = nn.MSELoss()(output, target_tensor.squeeze(0))
                loss.backward()
                optimizer.step()

        except Exception as e:
            print(f"âŒ ìœˆë„ìš° {i} ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    # price/return ëª¨ë‘ ì €ì¥
    save_predictions_to_csv(all_predictions, test_dates, folder_name, target=target)
    return all_predictions

def setup_model(input_dim, use_entmax=False):
    """ëª¨ë¸ ì„¤ì •"""
    print(f"â³ ëª¨ë¸ ì„¤ì • ì¤‘...")
    
    hidden_dim = 128
    output_dim = 28  # 14ì¼ ì˜ˆì¸¡
    num_layers = 2
    dropout = 0.2
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}")
    
    # Entmax ë˜ëŠ” Softmax ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
    if use_entmax:
        print("ğŸ” Entmax Attention ëª¨ë¸ ì‚¬ìš©")
        model = LSTMAttentionEntmax(input_dim, hidden_dim, output_dim, num_layers, dropout, use_entmax=True)
    else:
        print("ğŸ” Softmax Attention ëª¨ë¸ ì‚¬ìš©")
        model = LSTMAttentionSoftmax(input_dim, hidden_dim, output_dim, num_layers, dropout)
    
    print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ: {model.__class__.__name__}")
    
    return model, device

def run_sliding_window_prediction(model, test_data, scaler, seq_length, pred_length, device, stride=1, folder_name=None, test_dates=None, isOnline=False, target='price'):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ ì§„í–‰ (isOnline íŒŒë¼ë¯¸í„° ì¶”ê°€, target: price/return)"""
    print(f"â³ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì˜ˆì¸¡ ì§„í–‰ ì¤‘...")
    sliding_predictions = sliding_window_prediction(
        model, 
        test_data, 
        scaler, 
        seq_length, 
        pred_length, 
        stride=stride, 
        device=device, 
        test_dates=test_dates, 
        folder_name=folder_name,
        isOnline=isOnline,
        target=target
    )
    if not sliding_predictions:
        print("âš ï¸ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    print(f"âœ… ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì˜ˆì¸¡ ì™„ë£Œ: {len(sliding_predictions)}ê°œ ì˜ˆì¸¡")
    return sliding_predictions

